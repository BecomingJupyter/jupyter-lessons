{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce08d8f6",
   "metadata": {},
   "source": [
    "## Lesson 6: Running a simple HPC Job\n",
    "\n",
    "\n",
    " List of CLI commands\n",
    "This page lists basic CLI commands needed to work with directories and files on an ISU HPC cluster. These commands are described in more details at the end of the guide.\n",
    "\n",
    "man command - Show manual for command\n",
    "\n",
    "pwd - Prints the full name (the full path) of current/working directory\n",
    "\n",
    "ls - List directory contents\n",
    "ls -a - List all the content, including hidden files\n",
    "ls -l - List the content and its information\n",
    "\n",
    "mkdir foldername – Create a new directory foldername\n",
    "\n",
    "cd foldername – Change the working directory to foldername\n",
    "cd - Return to $HOME directory\n",
    "cd .. - Go up a directory\n",
    "cd - - Return to the previous directory\n",
    "\n",
    "emacs, nano, vi – File editors\n",
    "\n",
    "cp source destination – Copy source to destination\n",
    "cp -r source destination – Copy a directory recursively from source to destination\n",
    "\n",
    "mv source destination - Move (or rename) a file from source to destination\n",
    "\n",
    "rm file1 - Remove file1\n",
    "rm -r folder - Remove a directory and its contents recursively\n",
    "\n",
    "cat file – Print contents of file on the screen\n",
    "less file - View and paginate file\n",
    "head file - Show first 10 lines of file\n",
    "tail file - Show last 10 lines of file\n",
    "\n",
    "SLURM\n",
    "Since there may be many users simulteniously logged into cluster headnode, it's important not to run intensive tasks on the headnode. Such tasks should be performed on compute nodes.\n",
    "\n",
    "The headnode should be used to edit files and to submit jobs to a workload manager which schedules jobs to run on compute nodes. At ISU we use Slurm Workload Manager.\n",
    "\n",
    "Jobs can be run in interactive and batch modes. In interactive mode you will be logged (via a Slurm command) onto a compute node. Slurm will allocate requested resources only to your interactive job. You can request as little as 1 core, or multiple nodes, depending on what is needed by your job. Clearly, the more resources (cores, memory, CPU time, etc.) you request the longer you may need to wait for those recourses to become available. Since other jobs won't have access to the resources allocated to your job, it will look like a personal computer/cluster. Your environment, such as loaded environment modules, will be copied to the interactive session. Program/command output will be printed on the screen. Interactive mode should be used mostly to debug jobs. The downside of interactive mode: 1. requested resources may not be available right away, and users will have to wait for those before they will be able to run interactive commands 2. if connection to the cluster is lost, the interactive job will be killed by Slurm.\n",
    "\n",
    "For production jobs the batch mode should be used. In batch mode the resource requests and list of commands to be executed on compute nodes are placed in a job script, which is submitted to Slurm with sbatch command. Program/command output will be redirected to a file. To make it easier for users to create job scripts, we provide job script generators for each cluster -  see the appropriate cluster User Guide listed on the left.\n",
    "\n",
    "When an interactive or batch job is submitted to Slurm, the workload manager places job in the queue. Each job is given a priority which may change during the time the job stays in the queue. We use Slurm's fair-share scheduling on clusters at ISU. Job priority depends on how much resources had been used by the user or user's group, group's contribution to the cluster and how long the job has been waiting for resources. In accordance with job priority and amount of resources requested versus available, Slurm decides which resources to allocate to jobs. \n",
    "\n",
    "On research clusters you can use slurm-usage.py command to see your group usage. To see available options issue \"slurm-usage.py -h\" command.\n",
    "\n",
    "\n",
    "cp /shared/hpc/sample-job-scripts/abaqus/abaqus-2017.sh /work/<your group/path>\n",
    "\n",
    "Once this file has been copied to the same location as your input file, you will have to edit the \"parameter\" section of this script (labeled by the #  comments). \n",
    "\n",
    "When the changes are saved run the command: chmod +x abaqus-2017.sh\n",
    "\n",
    "this makes the file executable, then submit your job by executing the script with command:\n",
    "\n",
    "./abaqus-2017.sh\n",
    "\n",
    "sample script below:\n",
    "\n",
    "#!/bin/bash\n",
    "# \n",
    "# Sample script that creates an sbatch script that submits an Abaqus 2017 job to\n",
    "# SLURM.\n",
    "# Instructions:\n",
    "#   1.  Modify the items under PARAMETERS below. Save the file. \n",
    "#   2.  Make this file executable (e.g. chmod +x abaqus-2017.sh)\n",
    "#   3.  Run this file  (.e.g   ./abaqus-2017.sh ) \n",
    "\n",
    "# PARAMETERS:  (modify as needed)\n",
    "JOBNAME=abaqus-job1\n",
    "WORKDIR=/work/some-group/some-project\n",
    "INPUTFILE=input.inp\n",
    "# the USERFILE value is optional.  It is used to provide the name of a user-supplied routine.\n",
    "# If you need it, uncomment out the line below.\n",
    "\n",
    "#USERFILE=my-subroutine.for\n",
    "\n",
    "PARTITION=compute\n",
    "NUM_NODES=2\n",
    "PROCS_PER_NODE=16\n",
    "MAX_TIME=3:00:00\n",
    "EMAIL=some-netid@iastate.edu\n",
    "MAIL_TYPES=BEGIN,FAIL,END\n",
    "# end of PARAMETERS   \n",
    "\n",
    "TOTAL_PROCS=$((NUM_NODES*PROCS_PER_NODE))\n",
    "INPUTFILEPATH=${INPUTFILE}\n",
    "ERROR_FILE=${JOBNAME}.%j.error\n",
    "OUTPUT_FILE=${JOBNAME}.%j.output\n",
    "\n",
    "# if a USERFILE is specified, set USERSTRING and call abaqus with it.\n",
    "if [[ ${USERFILE} != \"\" ]]; then\n",
    "   USERSTRING=\"user=${USERFILE}\"\n",
    "fi\n",
    "\n",
    "# Everything below from 'cat ..' until END_OF_SCRIPT gets passed to sbatch.  Edit carefully.\n",
    "# Note that the regular shell variables (i.e.  $var,  ${var} ) are \n",
    "# filled in by bash when you run this script.\n",
    "# The escaped variables (i.e.  \\$var ) are filled in by SLURM at run time.\n",
    "cat <<END_OF_SCRIPT > ${JOBNAME}.sbatch\n",
    "#!/bin/bash\n",
    "#SBATCH -J $JOBNAME\n",
    "#SBATCH -D $WORKDIR\n",
    "#SBATCH -N $NUM_NODES\n",
    "#SBATCH -n $TOTAL_PROCS\n",
    "#SBATCH --partition=$PARTITION\n",
    "#SBATCH --ntasks-per-node=$PROCS_PER_NODE\n",
    "# it's a good idea to tell SLURM to use a large amount of memory. 120000 = 120GB.\n",
    "#SBATCH --mem=120000\n",
    "#SBATCH --time=$MAX_TIME\n",
    "####SBATCH -C compute\n",
    "#SBATCH --error=$ERROR_FILE\n",
    "#SBATCH --output=$OUTPUT_FILE\n",
    "#SBATCH --mail-type=$MAIL_TYPES\n",
    "#SBATCH --mail-user=$EMAIL\n",
    "cd $WORKDIR\n",
    "\n",
    "# Load the Intel compiler and Abaqus software.\n",
    "module load intel/17.4\n",
    "module load abaqus/2017\n",
    "\n",
    "# Abaqus doesn't support SLURM natively.  So, the script below gets the list of\n",
    "# allocated hosts from SLURM and uses it to construct the mp_host_list[] variable.  \n",
    "# It copies the global custom_v6.env file from the global Abaqus \"site\" directory and \n",
    "# adds the mp_host_list[] line to the bottom of the abaqus_v6.env file in the current folder.\n",
    "\n",
    "create_abaqus_mp_host_list.sh\n",
    "\n",
    "unset SLURM_GTIDS\n",
    "\n",
    "export I_MPI_HYDRA_BOOTSTRAP=ssh\n",
    "\n",
    "abaqus interactive analysis job=${JOBNAME} input=${INPUTFILEPATH} cpus=${TOTAL_PROCS} mp_mode=mpi memory=\"80 %\" ${USERSTRING} scratch=${WORKDIR}\n",
    "\n",
    "END_OF_SCRIPT\n",
    "\n",
    "# Now send the sbatch script created above to sbatch..\n",
    "echo \"running: sbatch ./${JOBNAME}.sbatch\"\n",
    "sbatch ./${JOBNAME}.sbatch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
